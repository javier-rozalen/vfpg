# -*- coding: utf-8 -*-
"""
Created on Tue Apr 19 00:02:23 2022

@author: javir
"""
import torch
from torch import nn
from torch.distributions.categorical import Categorical

class VFPG(nn.Module):
    """
    Variational Feynman Path Generator

    Parameters
    ----------
    N : int
        length of the sequence
    input_size : int
        dimension of each vector of the input sequence (2D in the paper)
    hidden_size : int
        length of the output vector of the LSTM, h
    num_layers : int, optional
        number of stacked LSTM layers. The default is 1.
    Nc : int, optional
        number of gaussian components for the GMM. The default is 3.
    Dense : bool, optional
        If true, adds a Linear layer that processes the output vector of 
        the LSTM and it returns the GMM parameters. The default is False.

    Returns
    -------
    x : list
        path generated by the VFPG by sampling from q.
    q_params : dict
        parameters of GMM at each time step.

    """
    def __init__(self,N,input_size,hidden_size,num_layers=1,Nc=3,Dense=False):
        super(VFPG, self).__init__()
        
        self.N = N
        self.Nc = Nc
        self.input_size = input_size
        self.Dense_bool = Dense
        
        # Layers    
        self.lstm = nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)
        if self.Dense_bool:
            self.Dense = nn.Linear(in_features=hidden_size,out_features=3*Nc)
        self.softmax = nn.Softmax(dim=0)
        self.softplus = nn.Softplus()
        
    def forward(self,z):
        inputs = torch.cat([z for _ in range(self.N)]).view(self.N,1,-1) # Repeat
        lstm_out, _ = self.lstm(inputs)
        x,q_params = [],{'gamma':[],'mu':[],'sigma':[]}
        c = 0
        for lstm_out_i in lstm_out:
            # GMM parameters
            GMM_params = self.Dense(lstm_out_i) if self.Dense_bool else lstm_out_i
         
            # Adequate transformations
            gamma = self.softmax(GMM_params[0][:self.Nc]) # 0<=gamma<=1, sum(gamma)=1
            mu = GMM_params[0][self.Nc:2*self.Nc]
            sigma = self.softplus(GMM_params[0][2*self.Nc:3*self.Nc]) # sigma>0
            
            # We store the parameters
            q_params['gamma'].append(gamma)
            q_params['mu'].append(mu)
            q_params['sigma'].append(sigma)

            # Sampling from mixture density
            gamma_cat_dist = Categorical(gamma)
            g_sampled_idx = gamma_cat_dist.sample()

            g = torch.randn(1,dtype=torch.float32)
            x_k = sigma[g_sampled_idx]*g+mu[g_sampled_idx]
            x.append(x_k.detach())

            c += 1
            
        return x,q_params

####################### TESTS #######################
if __name__ == '__main__':    
    print('Helloooo')
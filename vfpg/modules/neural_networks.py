
# -*- coding: utf-8 -*-

import torch
from torch import nn
from torch.distributions.categorical import Categorical
from torch.distributions.multivariate_normal import MultivariateNormal
from torch.distributions.normal import Normal
import numpy as np

toggle_prints = False

def print_(*message):
    if toggle_prints:
        print(*message)

class VFPG_old(nn.Module):
    """
    Variational Feynman Path Generator

    Parameters
    ----------
    N : int
        length of the sequence
    input_size : int
        dimension of each vector of the input sequence (2D in the paper)
    hidden_size : int
        length of the output vector of the LSTM, h
    num_layers : int, optional
        number of stacked LSTM layers. The default is 1.
    Nc : int, optional
        number of gaussian components for the GMM. The default is 3.
    Dense : bool, optional
        If true, adds a Linear layer that processes the output vector of 
        the LSTM and it returns the GMM parameters. The default is False.

    Returns
    -------
    x : list
        path generated by the VFPG by sampling from q.
    q_params : dict
        parameters of GMM at each time step.

    """
    def __init__(self,N,input_size,hidden_size,num_layers=1,Nc=3,Dense=False):
        super(VFPG_old, self).__init__()
        
        self.N = N
        self.Nc = Nc
        self.input_size = input_size
        self.Dense_bool = Dense
        
        # Layers    
        self.lstm = nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)
        if self.Dense_bool:
            self.Dense = nn.Linear(in_features=hidden_size,out_features=3*Nc)
        self.softmax = nn.Softmax(dim=0)
        self.softplus = nn.Softplus()
        
    def forward(self,z):
        inputs = torch.cat([z for _ in range(self.N)]).view(self.N,1,-1) # Repeat
        lstm_out, _ = self.lstm(inputs)
        x,q_params = [],{'gamma':[],'mu':[],'sigma':[]}
        c = 0
        for lstm_out_i in lstm_out:
            # GMM parameters
            GMM_params = self.Dense(lstm_out_i) if self.Dense_bool else lstm_out_i
         
            # Adequate transformations
            gamma = self.softmax(GMM_params[0][:self.Nc]) # 0<=gamma<=1, sum(gamma)=1
            mu = GMM_params[0][self.Nc:2*self.Nc]
            sigma = self.softplus(GMM_params[0][2*self.Nc:3*self.Nc]) # sigma>0
            
            # We store the parameters
            q_params['gamma'].append(gamma)
            q_params['mu'].append(mu)
            q_params['sigma'].append(sigma)

            # Sampling from mixture density
            gamma_cat_dist = Categorical(gamma)
            g_sampled_idx = gamma_cat_dist.sample()

            g = torch.randn(1,dtype=torch.float32)
            x_k = sigma[g_sampled_idx]*g+mu[g_sampled_idx]
            x.append(x_k.detach())

            c += 1
            
        return x, q_params

###############################################################################
class VFPG_suboptimal(nn.Module):

    def __init__(self, M, N, input_size, nhid, hidden_size, out_size, 
                 num_layers, Dense=True):
        super(VFPG_suboptimal, self).__init__()
        
        # Auxiliary stuff
        self.M = M
        self.N = N # length of each path
        self.input_size = input_size
        self.nhid = nhid # number of hidden neurons 
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.Nc = int(out_size / (input_size + 2))
        self.Dense = Dense
        self.pi = torch.tensor(np.pi)
        self.softmax = nn.Softmax(dim=0)
        
        # Neural Network
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True)
        self.fc = nn.Linear(in_features=hidden_size, 
                            out_features=out_size,                                                   
                            bias=True)

    def GMM(self, params):    
        # params.shape = [M,(N+2)*Nc]
        N = self.input_size
        
        gammas, mus, sigmas = [], [], []
        for i in range(self.Nc):
            gc = params[:, i*(N+2):(i+1)*(N+2)]
            params_mu = gc[:, :N] # [M,Nc]
            params_gamma = gc[:, N:N+1] # [M,Nc*N]
            params_sigma = gc[:, N+1:N+2] # [M,Nc]
            gammas.append(params_gamma)
            mus.append(params_mu)
            sigmas.append(params_sigma)
        
        #print(f'Gammas before softmax: {torch.stack(gammas)}')
        gammas = self.softmax(torch.stack(gammas))
        #print(f'Gammas after softmax: {gammas}')
        mus = torch.stack(mus)
        sigmas = torch.exp(torch.stack(sigmas))

        return gammas, mus, sigmas 
    
    def GMM_sampler(self, gammas, mus, sigmas):
        
        # Sampling from the mixture density
        _gammas = gammas.squeeze(2).transpose(0, 1)
        #print(f'_gammas: {_gammas}')
        _mus = mus.squeeze(2).transpose(0, 1)
        _sigmas = sigmas.squeeze(2).transpose(0, 1)
        gamma_cat_dist = Categorical(_gammas)
        g_sampled_idcs = gamma_cat_dist.sample()
        _chosen_mus = []
        _chosen_sigmas = []
        for i, idx in zip(range(self.M), g_sampled_idcs):
            _chosen_mus.append(_mus[i][idx].unsqueeze(0))
            _chosen_sigmas.append(_sigmas[i][idx].unsqueeze(0))
        _chosen_mus = torch.stack(_chosen_mus)
        _chosen_sigmas = torch.stack(_chosen_sigmas)
        #print(f'_chosen_mus: {_chosen_mus}')
        gmm_dist = Normal(loc=_chosen_mus, scale=_chosen_sigmas)
        x_pred = gmm_dist.sample()
    
        # Computing the probability of the sample
        x_pred_rep = x_pred.repeat(1, self.Nc)
        kernels_exponent = (x_pred_rep - _mus)**2
        kernels = torch.exp(-0.5*kernels_exponent) / (_sigmas**2)
        kernels /= ((2*self.pi)**(self.N/2))*(_sigmas**self.N)
        
        gammas = gammas.view(self.M, 1, self.Nc)
        kernels = kernels.view(self.M, self.Nc, 1)
        x_cond_prob = torch.bmm(gammas, kernels).squeeze(2)
        """
        print(f'Gammas: {_gammas}', _gammas.size())
        print(f'Mus: {_mus}', _mus.size())
        print(f'Sigmas: {_sigmas}', _sigmas.size())
        print(f'Indeces of chosen gammas (categorical): {g_sampled_idcs}')
        print(f'Chosen mus: {_chosen_mus}')
        print(f'Chosen sigmas: {_chosen_sigmas}')
        print(f'Repeated x_pred: {x_pred_rep}', x_pred_rep.size())
        print(f'Exponent: {kernels_exponent}', kernels_exponent.size())
        print(f'Kernels: {kernels}', kernels.size())
        print(f'x_pred: {x_pred}')
        print(f'Returned x_preds: {x_pred.unsqueeze(2)}')
        print(f'x_cond_probs: {x_cond_prob}')
        """
        return x_pred.unsqueeze(2), x_cond_prob 
        
    def forward(self, x_prev):
        preds, cond_probs = [x_prev], [torch.ones(self.M, 1)]
        
        # Initial cell states
        h_t = torch.zeros(self.num_layers, self.M, self.hidden_size)
        c_t = torch.zeros(self.num_layers, self.M, self.hidden_size)
        
        # Recurrence loop
        for i in range(self.N - 2):
            """
            print(f'\nIteration {i+1}/{self.N}\n-------------------')
            print(f'x_prev: {x_prev}', x_prev.size())
            print(f'h_t: {h_t}', h_t.size())
            print(f'c_t: {c_t}', c_t.size())
            """
            h_last_layer, (h_t, c_t) = self.lstm(x_prev, (h_t, c_t))
            y = self.fc(h_last_layer) if self.Dense else h_last_layer
            gammas, mus, sigmas = self.GMM(y.squeeze(1))
            x_pred, x_cond_prob = self.GMM_sampler(gammas, mus, sigmas)
            preds.append(x_pred)
            cond_probs.append(x_cond_prob)
            x_prev = x_pred 
        
        # We close the paths
        preds.append(preds[0])
        cond_probs.append(torch.ones(self.M, 1))
        paths = torch.stack(preds).squeeze().transpose(0,1)
        paths_cond_probs = torch.stack(cond_probs).squeeze().transpose(0,1)
        """
        print(f'Paths: {paths}')
        print(f'Paths_cond_probs: {paths_cond_probs}')
        """
        return paths, paths_cond_probs

class VFPG_ours(nn.Module):

    def __init__(self, dev, M, N, input_size, nhid, hidden_size, out_size, 
                 num_layers, Dense=True):
        super(VFPG_ours, self).__init__()
        
        # Auxiliary stuff
        self.dev = dev
        self.M = M
        self.N = N # length of each path
        self.input_size = input_size
        self.nhid = nhid # number of hidden neurons 
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.Nc = int(out_size / (1 + 2))
        self.Dense = Dense
        self.pi = torch.tensor(np.pi)
        self.softmax = nn.Softmax(dim=1)
        
        # Neural Network
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True)

        self.fc = nn.Linear(in_features=hidden_size, 
                            out_features=out_size,                                                   
                            bias=True)

    def GMM(self, params):    
        # params size = [M, (N+2)*Nc]
        
        gammas = self.softmax(params[:, :self.Nc]) # size [M, Nc]
        sigmas = torch.exp(params[:, self.Nc:2*self.Nc]) # size [M, Nc]
        mus = params[:, 2*self.Nc:] # size [M, N*Nc]
        """
        print(f'gammas: {gammas}', gammas.shape)
        print(f'sigmas: {sigmas}', sigmas.shape)
        print(f'mus: {mus}', mus.shape)
        """
        return gammas, mus, sigmas 
    
    def GMM_sampler(self, gammas, mus, sigmas):
        
        # Sampling from the mixture density
        g_sampled_idcs = Categorical(gammas).sample().unsqueeze(1)
        chosen_mus = mus.gather(1, g_sampled_idcs)
        chosen_sigmas = sigmas.gather(1, g_sampled_idcs)
        gmm_dist = Normal(loc=chosen_mus, scale=chosen_sigmas)
        x_pred = gmm_dist.sample()
    
        # Computing the probability of the sample
        kernels_exponent = (x_pred.repeat(1, self.Nc) - mus)**2
        kernels = torch.exp(-0.5*kernels_exponent) / (sigmas**2)
        kernels /= ((2*self.pi)**(self.N/2))*(sigmas**self.N)
        
        gammas = gammas.view(self.M, 1, self.Nc)
        kernels = kernels.view(self.M, self.Nc, 1)
        x_cond_prob = (gammas @ kernels).squeeze(2)

        return x_pred.unsqueeze(2), x_cond_prob 
        
    def forward(self, x_prev):
        preds, cond_probs = [x_prev], [torch.ones(self.M, 1).to(self.dev)]

        # Initial cell states
        h_t = torch.zeros(self.num_layers, self.M, self.hidden_size).to(self.dev)
        c_t = torch.zeros(self.num_layers, self.M, self.hidden_size).to(self.dev)
        
        # Recurrence loop
        for i in range(self.N - 2):
            h_last_layer, (h_t, c_t) = self.lstm(x_prev, (h_t, c_t))
            y = self.fc(h_last_layer) if self.Dense else h_last_layer
            gammas, mus, sigmas = self.GMM(y.squeeze(1))

            x_pred, x_cond_prob = self.GMM_sampler(gammas, mus, sigmas)
            #print(f'x_pred: {x_pred}', x_pred.shape)
            preds.append(x_pred)
            cond_probs.append(x_cond_prob)
            x_prev = x_pred
            x_prev = torch.randn(self.M, 1, self.input_size) 
        
        # We close the paths
        preds.append(preds[0])
        cond_probs.append(cond_probs[0])
        paths = torch.stack(preds).squeeze().transpose(0,1)
        paths_cond_probs = torch.stack(cond_probs).squeeze().transpose(0,1)
        #print(f'paths: {paths}', paths.shape)
        #print(f'paths_cond_probs: {paths_cond_probs}', paths_cond_probs.shape)

        return paths, paths_cond_probs, mus, sigmas
    
class VFPG_theirs(nn.Module):

    def __init__(self, dev, M, N, input_size, nhid, hidden_size, out_size, 
                 num_layers, Dense=True):
        super(VFPG_theirs, self).__init__()
        
        # Auxiliary stuff
        self.dev = dev
        self.M = M # batch size
        self.N = N # length of each path
        self.input_size = input_size
        self.nhid = nhid # number of hidden neurons 
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.Nc = int(out_size / (1 + 2))
        self.Dense = Dense
        self.pi = torch.tensor(np.pi)
        self.softmax = nn.Softmax(dim=2)
        
        # Neural Network
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True)
        self.fc = nn.Linear(in_features=hidden_size, 
                            out_features=out_size,                                                   
                            bias=True)

    def GMM(self, params):    
        # params size = [M, N, (N+2)*Nc]
        gammas = self.softmax(params[:, :, :self.Nc]) # size [M, N, Nc]
        sigmas = torch.exp(params[:, :, self.Nc:2*self.Nc]) # size [M, N, Nc]
        mus = params[:, :, 2*self.Nc:] # size [M, N, N*Nc]
        """
        print(f'gammas: {gammas}', gammas.shape)
        print(f'sigmas: {sigmas}', sigmas.shape)
        print(f'mus: {mus}', mus.shape)
        """
        return gammas, mus, sigmas 
    
    def GMM_sampler(self, gammas, mus, sigmas):
        
        # Sampling from the mixture density
        g_sampled_idcs = Categorical(gammas).sample().unsqueeze(2)
        #print(f'g_sampled_idcs: {g_sampled_idcs}', g_sampled_idcs.shape)
        chosen_mus = mus.gather(2, g_sampled_idcs)
        chosen_sigmas = sigmas.gather(2, g_sampled_idcs)
        #print(f'chosen_mus: {chosen_mus}', chosen_mus.shape)
        """
        gmm_dist = Normal(loc=chosen_mus, scale=chosen_sigmas)
        x_pred = gmm_dist.sample()
        """
        g = torch.randn(1).to(self.dev)
        x_pred = chosen_mus + g*chosen_sigmas
        #print(f'x_pred: {x_pred}', x_pred.shape)
    
        # Computing the probability of the sample
        #print(f'x_pred_rep: {x_pred.repeat(1, 1, self.Nc)}', x_pred.repeat(1, 1, self.Nc).shape)
        kernels_exponent = (x_pred.repeat(1, 1, self.Nc) - mus)**2
        kernels = torch.exp(-0.5*kernels_exponent) / (sigmas**2)
        kernels /= ((2*self.pi)**(self.N/2))*(sigmas**self.N)
        
        _gammas = gammas.view(self.M, self.N, 1, self.Nc)
        _kernels = kernels.view(self.M, self.N, self.Nc, 1)
        #print(f'_gammas: {_gammas}', _gammas.shape)
        #print(f'_kernels: {_kernels}', _kernels.shape)
        x_cond_prob = (_gammas @ _kernels).squeeze(3).squeeze(2)
        #print(f'x_cond_prob: {x_cond_prob}', x_cond_prob.shape)

        return x_pred, x_cond_prob
        
    def forward(self, z):
        
        # Initial cell states
        h_t = torch.zeros(self.num_layers, self.M, self.hidden_size).to(self.dev)
        c_t = torch.zeros(self.num_layers, self.M, self.hidden_size).to(self.dev)
        
        h_last_layer, (h_t, c_t) = self.lstm(z, (h_t, c_t)) # LSTM
        """
        print(f'h_last_layer: {h_last_layer}', h_last_layer.shape)
        print(f'h_t: {h_t}', h_t.shape)
        print(f'c_t: {c_t}', c_t.shape)
        """
        y = self.fc(h_last_layer) if self.Dense else h_last_layer # Linear
        #print(f'y: {y}', y.shape)
        gammas, mus, sigmas = self.GMM(y) # mixture params computation
        paths, paths_cond_probs = self.GMM_sampler(gammas, mus, sigmas) # samples
        """
        print(f'gammas: {gammas}', gammas.shape)
        print(f'first gammas: {gammas[:, 0, :]}', gammas[:, 0, :].shape)
        """
        return paths, paths_cond_probs, mus, sigmas
    
class VFPG_theirs_v2(nn.Module):

    def __init__(self, dev, batch_size, N, input_size, nhid, hidden_size, out_size, 
                 num_layers, Dense=True, dropout=0):
        super(VFPG_theirs_v2, self).__init__()
        
        # Auxiliary stuff
        self.dev = dev
        self.batch_size = batch_size # batch size
        self.Nc = self.batch_size # number of gaussian components
        self.N = N # length of each path
        self.input_size = input_size
        self.nhid = nhid # number of hidden neurons 
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.Dense_bool = Dense
        self.pi = torch.tensor(np.pi)
        self.softmax = nn.Softmax(dim=0)
        self.softplus = nn.Softplus()
        
        # Neural Network
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True,
                            dropout=dropout)
        self.fc = nn.Linear(in_features=hidden_size, 
                            out_features=out_size,                                                   
                            bias=True)
        self.activation = nn.Softplus()
        self.Dense = nn.Sequential(self.fc, self.activation)
        
    def GMM(self, params):    
        # params.shape = [batch_size, N, 3]
        gammas = self.softmax(params[:, :, 0]) # size [batch_size, N]
        mus = params[:, :, 1] # size [batch_size, N]
        sigmas = self.softplus(params[:, :, 2]) # size [batch_size, N]
        
        print_(f'gammas: {gammas}', gammas.shape)
        print_(f'mus: {mus}', mus.shape)
        print_(f'sigmas: {sigmas}', sigmas.shape)
        
        return gammas, mus, sigmas 
    
    def sample(self, gammas, mus, sigmas):
        # Sampling from the mixture density
        _gammas_categ = gammas.transpose(0,1) # size [N, batch_size]
        _mus_categ = mus.transpose(0,1) # size [N, batch_size]
        _sigmas_categ = sigmas.transpose(0,1) # size [N, batch_size]
        print_(f'_gammas_categ: {_gammas_categ}', _gammas_categ.shape)
        print_(f'_mus_categ: {_mus_categ}', _mus_categ.shape)
        print_(f'_sigmas_categ: {_sigmas_categ}', _sigmas_categ.shape)
        g_sampled_idcs = Categorical(_gammas_categ).sample().unsqueeze(1)
        print_(f'g_sampled_idcs: {g_sampled_idcs}', g_sampled_idcs.shape)
        chosen_gammas = _gammas_categ.gather(1, g_sampled_idcs) # size [N, 1]
        chosen_mus = _mus_categ.gather(1, g_sampled_idcs) # size [N, 1]
        chosen_sigmas = _sigmas_categ.gather(1, g_sampled_idcs) # size [N, 1]
        print_(f'chosen _gammas_categ: {chosen_gammas}', chosen_gammas.shape)
        print_(f'chosen _mus_categ: {chosen_mus}', chosen_mus.shape)
        print_(f'chosen _sigmas_categ: {chosen_sigmas}', chosen_sigmas.shape)
        g = torch.randn(self.N, 1).to(self.dev) 
        x_pred = chosen_mus + g*chosen_sigmas # size [N, 1]
        print_(f'x_pred: {x_pred}', x_pred.shape)
    
        # Computing the probability of the sample
        print_(f'x_pred_rep: {x_pred.repeat(1, self.batch_size)}', 
              x_pred.repeat(1, self.batch_size).shape)
        x_pred_rep = x_pred.repeat(1, self.batch_size) # size [N, batch_size]
        kernels_exponent = (x_pred_rep - _mus_categ)**2
        kernels = torch.exp(-0.5*kernels_exponent) / (_sigmas_categ**2)
        kernels /= ((2*self.pi)**(self.N/2))*(_sigmas_categ**self.N) # size [N, batch_size]
        print_(f'kernels: {kernels}', kernels.shape)
        
        _gammas = gammas.view(self.N, 1, self.batch_size)
        _kernels = kernels.view(self.N, self.batch_size, 1)
        print_(f'_gammas: {_gammas}', _gammas.shape)
        print_(f'_kernels: {_kernels}', _kernels.shape)
        
        x_cond_prob = (_gammas @ _kernels).squeeze(2).squeeze(1)
        print_(f'x_pred final: {x_pred.squeeze()}', x_pred.squeeze().shape)
        print_(f'x_cond_prob final: {x_cond_prob}', x_cond_prob.shape)
        
        return x_pred.squeeze(), x_cond_prob
    
    def sample_tocho(self, M_MC, gammas, mus, sigmas):
        # Sampling from the mixture density
        
        # We add an extra dimension of size M_MC to sample M_MC paths "at once"
        gammas = gammas.repeat(M_MC, 1, 1)
        mus = mus.repeat(M_MC, 1, 1)
        sigmas = sigmas.repeat(M_MC, 1, 1)
        print_(f'extended gamas: {gammas}', gammas.shape)
        
        # We transpose [batch_size, N] --> [N, batch_size] because we need
        # to apply Categorical on gammas.
        _gammas_categ = gammas.transpose(1,2) # size [M_MC, N, batch_size]
        _mus_categ = mus.transpose(1,2) # size [M_MC, N, batch_size]
        _sigmas_categ = sigmas.transpose(1,2) # size [M_MC, N, batch_size]
        print_(f'_gammas_categ: {_gammas_categ}', _gammas_categ.shape)
        print_(f'_mus_categ: {_mus_categ}', _mus_categ.shape)
        print_(f'_sigmas_categ: {_sigmas_categ}', _sigmas_categ.shape)
        
        # We sample from the categorical gammas and keep the chosen mus, sigmas
        g_sampled_idcs = Categorical(_gammas_categ).sample().unsqueeze(2) # size [M_MC, N, 1]
        print_(f'g_sampled_idcs: {g_sampled_idcs}', g_sampled_idcs.shape)
        chosen_gammas = _gammas_categ.gather(2, g_sampled_idcs) # size [M_MC, N, 1]
        chosen_mus = _mus_categ.gather(2, g_sampled_idcs) # size [M_MC, N, 1]
        chosen_sigmas = _sigmas_categ.gather(2, g_sampled_idcs) # size [M_MC, N, 1]
        print_(f'chosen _gammas_categ: {chosen_gammas}', chosen_gammas.shape)
        print_(f'chosen _mus_categ: {chosen_mus}', chosen_mus.shape)
        print_(f'chosen _sigmas_categ: {chosen_sigmas}', chosen_sigmas.shape)
        
        # We use the chosen mus, sigmas to sample a path
        normal_dist = Normal(loc=chosen_sigmas, scale=chosen_mus)
        x_pred = normal_dist.rsample() # size [M_MC, N, 1]
        print_(f'x_pred: {x_pred}', x_pred.shape)
    
        # We compute the (conditional) probability of each step of the path
        print_(f'x_pred_rep: {x_pred.repeat(1, 1, self.batch_size)}', 
              x_pred.repeat(1, 1, self.batch_size).shape)
        x_pred_rep = x_pred.repeat(1, 1, self.batch_size) # size [M_MC, N, batch_size]
        kernels_exponent = (x_pred_rep - _mus_categ)**2
        kernels = torch.exp(-0.5*kernels_exponent) / (_sigmas_categ**2)
        kernels /= ((2*self.pi)**(self.N/2))*(_sigmas_categ**self.N) # size [M_MC, N, batch_size]
        print_(f'kernels: {kernels}', kernels.shape)
        
        _gammas = gammas.view(M_MC, self.N, 1, self.batch_size)
        _kernels = kernels.view(M_MC, self.N, self.batch_size, 1)
        print_(f'_gammas: {_gammas}', _gammas.shape)
        print_(f'_kernels: {_kernels}', _kernels.shape)
        
        x_cond_prob = (_gammas @ _kernels).squeeze(3).squeeze(2)
        print_(f'x_pred final: {x_pred.squeeze()}', x_pred.squeeze().shape)
        print_(f'x_cond_prob final: {x_cond_prob}', x_cond_prob.shape)
        
        return x_pred.squeeze(), x_cond_prob
        
    def forward(self, z):
        
        # Initial cell states
        h_t = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.dev)
        c_t = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.dev)
        
        h_last_layer, (h_t, c_t) = self.lstm(z, (h_t, c_t)) # LSTM
        
        print_(f'h_last_layer: {h_last_layer}', h_last_layer.shape)
        print_(f'h_t: {h_t}', h_t.shape)
        print_(f'c_t: {c_t}', c_t.shape)
        
        y = self.Dense(h_last_layer) if self.Dense_bool else h_last_layer # Linear
        # y.shape = [batch_size, N, 3]
        print_(f'y: {y}', y.shape)
        gammas, mus, sigmas = self.GMM(y) # mixture params computation

        return gammas, mus, sigmas

class VFPG_theirs_v3(nn.Module):

    def __init__(self, dev, batch_size, N, input_size, nhid, hidden_size, out_size, 
                 num_layers, Dense=True, dropout=0):
        super(VFPG_theirs_v2, self).__init__()
        
        # Auxiliary stuff
        self.dev = dev
        self.batch_size = batch_size # batch size
        self.Nc = self.batch_size # number of gaussian components
        self.N = N # length of each path
        self.input_size = input_size
        self.nhid = nhid # number of hidden neurons 
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.Dense_bool = Dense
        self.pi = torch.tensor(np.pi)
        self.softmax = nn.Softmax(dim=0)
        self.softplus = nn.Softplus()
        
        # Neural Network
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True,
                            dropout=dropout)
        self.fc = nn.Linear(in_features=hidden_size, 
                            out_features=out_size,                                                   
                            bias=True)
        self.activation = nn.Softplus()
        self.Dense = nn.Sequential(self.fc, self.activation)
        
    def GMM(self, params):    
        # params.shape = [batch_size, N, 3]
        gammas = self.softmax(params[:, :, 0]) # size [batch_size, N]
        mus = params[:, :, 1] # size [batch_size, N]
        sigmas2 = self.softplus(params[:, :, 2]) # size [batch_size, N]
        
        print_(f'gammas: {gammas}', gammas.shape)
        print_(f'mus: {mus}', mus.shape)
        print_(f'sigmas2: {sigmas2}', sigmas2.shape)
        
        return gammas, mus, sigmas2 
    
    def sample_tocho(self, M_MC, gammas, mus, sigmas):
        # Sampling from the mixture density
        
        # We add an extra dimension of size M_MC to sample M_MC paths "at once"
        gammas = gammas.repeat(M_MC, 1, 1)
        mus = mus.repeat(M_MC, 1, 1)
        sigmas = sigmas.repeat(M_MC, 1, 1)
        print_(f'extended gamas: {gammas}', gammas.shape)
        
        # We transpose [batch_size, N] --> [N, batch_size] because we need
        # to apply Categorical on gammas.
        _gammas_categ = gammas.transpose(1,2) # size [M_MC, N, batch_size]
        _mus_categ = mus.transpose(1,2) # size [M_MC, N, batch_size]
        _sigmas_categ = sigmas.transpose(1,2) # size [M_MC, N, batch_size]
        print_(f'_gammas_categ: {_gammas_categ}', _gammas_categ.shape)
        print_(f'_mus_categ: {_mus_categ}', _mus_categ.shape)
        print_(f'_sigmas_categ: {_sigmas_categ}', _sigmas_categ.shape)
        
        # We sample from the categorical gammas and keep the chosen mus, sigmas
        g_sampled_idcs = Categorical(_gammas_categ).sample().unsqueeze(2) # size [M_MC, N, 1]
        print_(f'g_sampled_idcs: {g_sampled_idcs}', g_sampled_idcs.shape)
        chosen_gammas = _gammas_categ.gather(2, g_sampled_idcs) # size [M_MC, N, 1]
        chosen_mus = _mus_categ.gather(2, g_sampled_idcs) # size [M_MC, N, 1]
        chosen_sigmas = _sigmas_categ.gather(2, g_sampled_idcs) # size [M_MC, N, 1]
        print_(f'chosen _gammas_categ: {chosen_gammas}', chosen_gammas.shape)
        print_(f'chosen _mus_categ: {chosen_mus}', chosen_mus.shape)
        print_(f'chosen _sigmas_categ: {chosen_sigmas}', chosen_sigmas.shape)
        
        # We use the chosen mus, sigmas to sample a path
        cov_mat = torch.diag_embed(chosen_sigmas)
        multivariate_normal_dist = Normal(loc=chosen_sigmas, 
                                          covariance_matrix=cov_mat)
        x_pred = multivariate_normal_dist.rsample() # size [M_MC, N, 1]
        print_(f'x_pred: {x_pred}', x_pred.shape)
    
        # We compute the (conditional) probability of each step of the path
        print_(f'x_pred_rep: {x_pred.repeat(1, 1, self.batch_size)}', 
              x_pred.repeat(1, 1, self.batch_size).shape)
        x_pred_rep = x_pred.repeat(1, 1, self.batch_size) # size [M_MC, N, batch_size]
        kernels_exponent = (x_pred_rep - _mus_categ)**2
        kernels = torch.exp(-0.5*kernels_exponent) / (_sigmas_categ**2)
        kernels /= ((2*self.pi)**(self.N/2))*(_sigmas_categ**self.N) # size [M_MC, N, batch_size]
        print_(f'kernels: {kernels}', kernels.shape)
        
        _gammas = gammas.view(M_MC, self.N, 1, self.batch_size)
        _kernels = kernels.view(M_MC, self.N, self.batch_size, 1)
        print_(f'_gammas: {_gammas}', _gammas.shape)
        print_(f'_kernels: {_kernels}', _kernels.shape)
        
        x_cond_prob = (_gammas @ _kernels).squeeze(3).squeeze(2)
        print_(f'x_pred final: {x_pred.squeeze()}', x_pred.squeeze().shape)
        print_(f'x_cond_prob final: {x_cond_prob}', x_cond_prob.shape)
        
        return x_pred.squeeze(), x_cond_prob
        
    def forward(self, z):
        
        # Initial cell states
        h_t = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.dev)
        c_t = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.dev)
        
        h_last_layer, (h_t, c_t) = self.lstm(z, (h_t, c_t)) # LSTM
        
        print_(f'h_last_layer: {h_last_layer}', h_last_layer.shape)
        print_(f'h_t: {h_t}', h_t.shape)
        print_(f'c_t: {c_t}', c_t.shape)
        
        y = self.Dense(h_last_layer) if self.Dense_bool else h_last_layer # Linear
        # y.shape = [batch_size, N, 3]
        print_(f'y: {y}', y.shape)
        gammas, mus, sigmas = self.GMM(y) # mixture params computation

        return gammas, mus, sigmas
####################### TESTS #######################
if __name__ == '__main__':    
    print('Helloooo')
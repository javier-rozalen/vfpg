# -*- coding: utf-8 -*-

import torch
from torch import nn
from torch.distributions.categorical import Categorical
from torch.distributions.normal import Normal
import numpy as np

class VFPG_old(nn.Module):
    """
    Variational Feynman Path Generator

    Parameters
    ----------
    N : int
        length of the sequence
    input_size : int
        dimension of each vector of the input sequence (2D in the paper)
    hidden_size : int
        length of the output vector of the LSTM, h
    num_layers : int, optional
        number of stacked LSTM layers. The default is 1.
    Nc : int, optional
        number of gaussian components for the GMM. The default is 3.
    Dense : bool, optional
        If true, adds a Linear layer that processes the output vector of 
        the LSTM and it returns the GMM parameters. The default is False.

    Returns
    -------
    x : list
        path generated by the VFPG by sampling from q.
    q_params : dict
        parameters of GMM at each time step.

    """
    def __init__(self,N,input_size,hidden_size,num_layers=1,Nc=3,Dense=False):
        super(VFPG, self).__init__()
        
        self.N = N
        self.Nc = Nc
        self.input_size = input_size
        self.Dense_bool = Dense
        
        # Layers    
        self.lstm = nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)
        if self.Dense_bool:
            self.Dense = nn.Linear(in_features=hidden_size,out_features=3*Nc)
        self.softmax = nn.Softmax(dim=0)
        self.softplus = nn.Softplus()
        
    def forward(self,z):
        inputs = torch.cat([z for _ in range(self.N)]).view(self.N,1,-1) # Repeat
        lstm_out, _ = self.lstm(inputs)
        x,q_params = [],{'gamma':[],'mu':[],'sigma':[]}
        c = 0
        for lstm_out_i in lstm_out:
            # GMM parameters
            GMM_params = self.Dense(lstm_out_i) if self.Dense_bool else lstm_out_i
         
            # Adequate transformations
            gamma = self.softmax(GMM_params[0][:self.Nc]) # 0<=gamma<=1, sum(gamma)=1
            mu = GMM_params[0][self.Nc:2*self.Nc]
            sigma = self.softplus(GMM_params[0][2*self.Nc:3*self.Nc]) # sigma>0
            
            # We store the parameters
            q_params['gamma'].append(gamma)
            q_params['mu'].append(mu)
            q_params['sigma'].append(sigma)

            # Sampling from mixture density
            gamma_cat_dist = Categorical(gamma)
            g_sampled_idx = gamma_cat_dist.sample()

            g = torch.randn(1,dtype=torch.float32)
            x_k = sigma[g_sampled_idx]*g+mu[g_sampled_idx]
            x.append(x_k.detach())

            c += 1
            
        return x, q_params

###############################################################################
class VFPG(nn.Module):

    def __init__(self, M, N, input_size, nhid, hidden_size, out_size, 
                 num_layers, Dense=True):
        super(VFPG, self).__init__()
        
        # Auxiliary stuff
        self.M = M
        self.N = N # length of each path
        self.input_size = input_size
        self.nhid = nhid # number of hidden neurons 
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.Nc = int(out_size / (input_size + 2))
        self.Dense = Dense
        self.pi = torch.tensor(np.pi)
        self.softmax = nn.Softmax(dim=0)
        
        # Neural Network
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True)
        self.fc = nn.Linear(in_features=hidden_size, 
                            out_features=out_size,                                                   
                            bias=True)

    def GMM(self, params):    
        # params.shape = [M,(N+2)*Nc]
        N = self.input_size
        
        gammas, mus, sigmas = [], [], []
        for i in range(self.Nc):
            gc = params[:, i*(N+2):(i+1)*(N+2)]
            params_mu = gc[:, :N] # [M,Nc]
            params_gamma = gc[:, N:N+1] # [M,Nc*N]
            params_sigma = gc[:, N+1:N+2] # [M,Nc]
            gammas.append(params_gamma)
            mus.append(params_mu)
            sigmas.append(params_sigma)
        
        #print(f'Gammas before softmax: {torch.stack(gammas)}')
        gammas = self.softmax(torch.stack(gammas))
        #print(f'Gammas after softmax: {gammas}')
        mus = torch.stack(mus)
        sigmas = torch.exp(torch.stack(sigmas))

        return gammas, mus, sigmas 
    
    def GMM_sampler(self, gammas, mus, sigmas):
        
        # Sampling from the mixture density
        _gammas = gammas.squeeze(2).transpose(0, 1)
        #print(f'_gammas: {_gammas}')
        _mus = mus.squeeze(2).transpose(0, 1)
        _sigmas = sigmas.squeeze(2).transpose(0, 1)
        gamma_cat_dist = Categorical(_gammas)
        g_sampled_idcs = gamma_cat_dist.sample()
        _chosen_mus = []
        _chosen_sigmas = []
        for i, idx in zip(range(self.M), g_sampled_idcs):
            _chosen_mus.append(_mus[i][idx].unsqueeze(0))
            _chosen_sigmas.append(_sigmas[i][idx].unsqueeze(0))
        _chosen_mus = torch.stack(_chosen_mus)
        _chosen_sigmas = torch.stack(_chosen_sigmas)
        #print(f'_chosen_mus: {_chosen_mus}')
        gmm_dist = Normal(loc=_chosen_mus, scale=_chosen_sigmas)
        x_pred = gmm_dist.sample()
    
        # Computing the probability of the sample
        x_pred_rep = x_pred.repeat(1, self.Nc)
        kernels_exponent = (x_pred_rep - _mus)**2
        kernels = torch.exp(-0.5*kernels_exponent) / (_sigmas**2)
        kernels /= ((2*self.pi)**(self.N/2))*(_sigmas**self.N)
        
        gammas = gammas.view(self.M, 1, self.Nc)
        kernels = kernels.view(self.M, self.Nc, 1)
        x_cond_prob = torch.bmm(gammas, kernels).squeeze(2)
        """
        print(f'Gammas: {_gammas}', _gammas.size())
        print(f'Mus: {_mus}', _mus.size())
        print(f'Sigmas: {_sigmas}', _sigmas.size())
        print(f'Indeces of chosen gammas (categorical): {g_sampled_idcs}')
        print(f'Chosen mus: {_chosen_mus}')
        print(f'Chosen sigmas: {_chosen_sigmas}')
        print(f'Repeated x_pred: {x_pred_rep}', x_pred_rep.size())
        print(f'Exponent: {kernels_exponent}', kernels_exponent.size())
        print(f'Kernels: {kernels}', kernels.size())
        print(f'x_pred: {x_pred}')
        print(f'Returned x_preds: {x_pred.unsqueeze(2)}')
        print(f'x_cond_probs: {x_cond_prob}')
        """
        return x_pred.unsqueeze(2), x_cond_prob 
        
    def forward(self, x_prev):
        preds, cond_probs = [x_prev], [torch.ones(self.M, 1)]
        
        # Initial cell states
        h_t = torch.zeros(self.num_layers, self.M, self.hidden_size)
        c_t = torch.zeros(self.num_layers, self.M, self.hidden_size)
        
        # Recurrence loop
        for i in range(self.N - 2):
            """
            print(f'\nIteration {i+1}/{self.N}\n-------------------')
            print(f'x_prev: {x_prev}', x_prev.size())
            print(f'h_t: {h_t}', h_t.size())
            print(f'c_t: {c_t}', c_t.size())
            """
            h_last_layer, (h_t, c_t) = self.lstm(x_prev, (h_t, c_t))
            y = self.fc(h_last_layer) if self.Dense else h_last_layer
            gammas, mus, sigmas = self.GMM(y.squeeze(1))
            x_pred, x_cond_prob = self.GMM_sampler(gammas, mus, sigmas)
            preds.append(x_pred)
            cond_probs.append(x_cond_prob)
            x_prev = x_pred 
        
        # We close the paths
        preds.append(preds[0])
        cond_probs.append(torch.ones(self.M, 1))
        paths = torch.stack(preds).squeeze().transpose(0,1)
        paths_cond_probs = torch.stack(cond_probs).squeeze().transpose(0,1)
        """
        print(f'Paths: {paths}')
        print(f'Paths_cond_probs: {paths_cond_probs}')
        """
        return paths, paths_cond_probs

class VFPG_optimized(nn.Module):

    def __init__(self, M, N, input_size, nhid, hidden_size, out_size, 
                 num_layers, Dense=True):
        super(VFPG_optimized, self).__init__()
        
        # Auxiliary stuff
        self.M = M
        self.N = N # length of each path
        self.input_size = input_size
        self.nhid = nhid # number of hidden neurons 
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.Nc = int(out_size / (input_size + 2))
        self.Dense = Dense
        self.pi = torch.tensor(np.pi)
        self.softmax = nn.Softmax(dim=1)
        
        # Neural Network
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True)
        self.fc = nn.Linear(in_features=hidden_size, 
                            out_features=out_size,                                                   
                            bias=True)

    def GMM(self, params):    
        # params size = [M, (N+2)*Nc]
        
        gammas = self.softmax(params[:, :self.Nc]) # size [M, Nc]
        sigmas = torch.exp(params[:, self.Nc:2*self.Nc]) # size [M, Nc]
        mus = params[:, 2*self.Nc:] # size [M, N*Nc]

        return gammas, mus, sigmas 
    
    def GMM_sampler(self, gammas, mus, sigmas):
        
        # Sampling from the mixture density
        g_sampled_idcs = Categorical(gammas).sample().unsqueeze(1)
        chosen_mus = mus.gather(1, g_sampled_idcs)
        chosen_sigmas = sigmas.gather(1, g_sampled_idcs)
        gmm_dist = Normal(loc=chosen_mus, scale=chosen_sigmas)
        x_pred = gmm_dist.sample()
    
        # Computing the probability of the sample
        x_pred_rep = x_pred.repeat(1, self.Nc)
        kernels_exponent = (x_pred_rep - mus)**2
        kernels = torch.exp(-0.5*kernels_exponent) / (sigmas**2)
        kernels /= ((2*self.pi)**(self.N/2))*(sigmas**self.N)
        
        gammas = gammas.view(self.M, 1, self.Nc)
        kernels = kernels.view(self.M, self.Nc, 1)
        x_cond_prob = torch.bmm(gammas, kernels).squeeze(2)

        return x_pred.unsqueeze(2), x_cond_prob 
        
    def forward(self, x_prev):
        preds, cond_probs = [x_prev], [torch.ones(self.M, 1)]
        
        # Initial cell states
        h_t = torch.zeros(self.num_layers, self.M, self.hidden_size)
        c_t = torch.zeros(self.num_layers, self.M, self.hidden_size)
        
        # Recurrence loop
        for i in range(self.N - 2):
            h_last_layer, (h_t, c_t) = self.lstm(x_prev, (h_t, c_t))
            y = self.fc(h_last_layer) if self.Dense else h_last_layer
            gammas, mus, sigmas = self.GMM(y.squeeze(1))
            x_pred, x_cond_prob = self.GMM_sampler(gammas, mus, sigmas)
            preds.append(x_pred)
            cond_probs.append(x_cond_prob)
            x_prev = x_pred 
        
        # We close the paths
        preds.append(preds[0])
        cond_probs.append(torch.ones(self.M, 1))
        paths = torch.stack(preds).squeeze().transpose(0,1)
        paths_cond_probs = torch.stack(cond_probs).squeeze().transpose(0,1)
        """
        print(f'Paths: {paths}')
        print(f'Paths_cond_probs: {paths_cond_probs}')
        """
        return paths, paths_cond_probs
####################### TESTS #######################
if __name__ == '__main__':    
    print('Helloooo')